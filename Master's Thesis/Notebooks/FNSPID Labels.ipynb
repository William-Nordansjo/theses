{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39135490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_large_csv_in_chunks(\n",
    "    file_path: str,\n",
    "    usecols: list[str] = None,\n",
    "    filter_col: str = None,\n",
    "    filter_values: list[str] = None,\n",
    "    chunksize: int = 100_000,\n",
    "    dropna_cols: list[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a large CSV file in chunks and returns a concatenated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    - usecols (list[str], optional): Columns to load.\n",
    "    - filter_col (str, optional): Column to apply filtering on.\n",
    "    - filter_values (list[str], optional): Values to keep in filter_col.\n",
    "    - chunksize (int): Number of rows per chunk.\n",
    "    - dropna_cols (list[str], optional): Drop rows with NaN in these columns.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered and loaded data in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = pd.read_csv(file_path, usecols=usecols, chunksize=chunksize)\n",
    "    chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(reader):\n",
    "        print(f\"ğŸ”„ Processing chunk {i + 1}\")\n",
    "        \n",
    "        if dropna_cols:\n",
    "            chunk = chunk.dropna(subset=dropna_cols)\n",
    "        \n",
    "        if filter_col and filter_values:\n",
    "            chunk = chunk[chunk[filter_col].isin(filter_values)]\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"âœ… Loaded {len(df):,} rows into memory.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ce0f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing chunk 1\n",
      "ğŸ”„ Processing chunk 2\n",
      "ğŸ”„ Processing chunk 3\n",
      "ğŸ”„ Processing chunk 4\n",
      "ğŸ”„ Processing chunk 5\n",
      "ğŸ”„ Processing chunk 6\n",
      "ğŸ”„ Processing chunk 7\n",
      "ğŸ”„ Processing chunk 8\n",
      "ğŸ”„ Processing chunk 9\n",
      "ğŸ”„ Processing chunk 10\n",
      "ğŸ”„ Processing chunk 11\n",
      "ğŸ”„ Processing chunk 12\n",
      "ğŸ”„ Processing chunk 13\n",
      "ğŸ”„ Processing chunk 14\n",
      "ğŸ”„ Processing chunk 15\n",
      "ğŸ”„ Processing chunk 16\n",
      "ğŸ”„ Processing chunk 17\n",
      "ğŸ”„ Processing chunk 18\n",
      "ğŸ”„ Processing chunk 19\n",
      "ğŸ”„ Processing chunk 20\n",
      "ğŸ”„ Processing chunk 21\n",
      "ğŸ”„ Processing chunk 22\n",
      "ğŸ”„ Processing chunk 23\n",
      "ğŸ”„ Processing chunk 24\n",
      "ğŸ”„ Processing chunk 25\n",
      "ğŸ”„ Processing chunk 26\n",
      "ğŸ”„ Processing chunk 27\n",
      "ğŸ”„ Processing chunk 28\n",
      "ğŸ”„ Processing chunk 29\n",
      "ğŸ”„ Processing chunk 30\n",
      "ğŸ”„ Processing chunk 31\n",
      "ğŸ”„ Processing chunk 32\n",
      "ğŸ”„ Processing chunk 33\n",
      "ğŸ”„ Processing chunk 34\n",
      "ğŸ”„ Processing chunk 35\n",
      "ğŸ”„ Processing chunk 36\n",
      "ğŸ”„ Processing chunk 37\n",
      "ğŸ”„ Processing chunk 38\n",
      "ğŸ”„ Processing chunk 39\n",
      "ğŸ”„ Processing chunk 40\n",
      "ğŸ”„ Processing chunk 41\n",
      "ğŸ”„ Processing chunk 42\n",
      "ğŸ”„ Processing chunk 43\n",
      "ğŸ”„ Processing chunk 44\n",
      "ğŸ”„ Processing chunk 45\n",
      "ğŸ”„ Processing chunk 46\n",
      "ğŸ”„ Processing chunk 47\n",
      "ğŸ”„ Processing chunk 48\n",
      "ğŸ”„ Processing chunk 49\n",
      "ğŸ”„ Processing chunk 50\n",
      "ğŸ”„ Processing chunk 51\n",
      "ğŸ”„ Processing chunk 52\n",
      "ğŸ”„ Processing chunk 53\n",
      "ğŸ”„ Processing chunk 54\n",
      "ğŸ”„ Processing chunk 55\n",
      "ğŸ”„ Processing chunk 56\n",
      "ğŸ”„ Processing chunk 57\n",
      "ğŸ”„ Processing chunk 58\n",
      "ğŸ”„ Processing chunk 59\n",
      "ğŸ”„ Processing chunk 60\n",
      "ğŸ”„ Processing chunk 61\n",
      "ğŸ”„ Processing chunk 62\n",
      "ğŸ”„ Processing chunk 63\n",
      "ğŸ”„ Processing chunk 64\n",
      "ğŸ”„ Processing chunk 65\n",
      "ğŸ”„ Processing chunk 66\n",
      "ğŸ”„ Processing chunk 67\n",
      "ğŸ”„ Processing chunk 68\n",
      "ğŸ”„ Processing chunk 69\n",
      "ğŸ”„ Processing chunk 70\n",
      "ğŸ”„ Processing chunk 71\n",
      "ğŸ”„ Processing chunk 72\n",
      "ğŸ”„ Processing chunk 73\n",
      "ğŸ”„ Processing chunk 74\n",
      "ğŸ”„ Processing chunk 75\n",
      "ğŸ”„ Processing chunk 76\n",
      "ğŸ”„ Processing chunk 77\n",
      "ğŸ”„ Processing chunk 78\n",
      "ğŸ”„ Processing chunk 79\n",
      "ğŸ”„ Processing chunk 80\n",
      "ğŸ”„ Processing chunk 81\n",
      "ğŸ”„ Processing chunk 82\n",
      "ğŸ”„ Processing chunk 83\n",
      "ğŸ”„ Processing chunk 84\n",
      "ğŸ”„ Processing chunk 85\n",
      "ğŸ”„ Processing chunk 86\n",
      "ğŸ”„ Processing chunk 87\n",
      "ğŸ”„ Processing chunk 88\n",
      "ğŸ”„ Processing chunk 89\n",
      "ğŸ”„ Processing chunk 90\n",
      "ğŸ”„ Processing chunk 91\n",
      "ğŸ”„ Processing chunk 92\n",
      "ğŸ”„ Processing chunk 93\n",
      "ğŸ”„ Processing chunk 94\n",
      "ğŸ”„ Processing chunk 95\n",
      "ğŸ”„ Processing chunk 96\n",
      "ğŸ”„ Processing chunk 97\n",
      "ğŸ”„ Processing chunk 98\n",
      "ğŸ”„ Processing chunk 99\n",
      "ğŸ”„ Processing chunk 100\n",
      "ğŸ”„ Processing chunk 101\n",
      "ğŸ”„ Processing chunk 102\n",
      "ğŸ”„ Processing chunk 103\n",
      "ğŸ”„ Processing chunk 104\n",
      "ğŸ”„ Processing chunk 105\n",
      "ğŸ”„ Processing chunk 106\n",
      "ğŸ”„ Processing chunk 107\n",
      "ğŸ”„ Processing chunk 108\n",
      "ğŸ”„ Processing chunk 109\n",
      "ğŸ”„ Processing chunk 110\n",
      "ğŸ”„ Processing chunk 111\n",
      "ğŸ”„ Processing chunk 112\n",
      "ğŸ”„ Processing chunk 113\n",
      "ğŸ”„ Processing chunk 114\n",
      "ğŸ”„ Processing chunk 115\n",
      "ğŸ”„ Processing chunk 116\n",
      "ğŸ”„ Processing chunk 117\n",
      "ğŸ”„ Processing chunk 118\n",
      "ğŸ”„ Processing chunk 119\n",
      "ğŸ”„ Processing chunk 120\n",
      "ğŸ”„ Processing chunk 121\n",
      "ğŸ”„ Processing chunk 122\n",
      "ğŸ”„ Processing chunk 123\n",
      "ğŸ”„ Processing chunk 124\n",
      "ğŸ”„ Processing chunk 125\n",
      "ğŸ”„ Processing chunk 126\n",
      "ğŸ”„ Processing chunk 127\n",
      "ğŸ”„ Processing chunk 128\n",
      "ğŸ”„ Processing chunk 129\n",
      "ğŸ”„ Processing chunk 130\n",
      "ğŸ”„ Processing chunk 131\n",
      "ğŸ”„ Processing chunk 132\n",
      "ğŸ”„ Processing chunk 133\n",
      "ğŸ”„ Processing chunk 134\n",
      "ğŸ”„ Processing chunk 135\n",
      "ğŸ”„ Processing chunk 136\n",
      "ğŸ”„ Processing chunk 137\n",
      "ğŸ”„ Processing chunk 138\n",
      "ğŸ”„ Processing chunk 139\n",
      "ğŸ”„ Processing chunk 140\n",
      "ğŸ”„ Processing chunk 141\n",
      "ğŸ”„ Processing chunk 142\n",
      "ğŸ”„ Processing chunk 143\n",
      "ğŸ”„ Processing chunk 144\n",
      "ğŸ”„ Processing chunk 145\n",
      "ğŸ”„ Processing chunk 146\n",
      "ğŸ”„ Processing chunk 147\n",
      "ğŸ”„ Processing chunk 148\n",
      "ğŸ”„ Processing chunk 149\n",
      "ğŸ”„ Processing chunk 150\n",
      "ğŸ”„ Processing chunk 151\n",
      "ğŸ”„ Processing chunk 152\n",
      "ğŸ”„ Processing chunk 153\n",
      "ğŸ”„ Processing chunk 154\n",
      "ğŸ”„ Processing chunk 155\n",
      "ğŸ”„ Processing chunk 156\n",
      "âœ… Loaded 15,549,298 rows into memory.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Volumes/T7/External Downloads/nasdaq_titles_fuzzy_rdy.csv\"\n",
    "\n",
    "df = load_large_csv_in_chunks(\n",
    "    usecols= [\"date\", \"article_title\", \"article_title_clean\", \"stock_symbol\"],\n",
    "    file_path=file_path,\n",
    "    chunksize = 100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25affe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15549298 entries, 0 to 15549297\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Dtype \n",
      "---  ------               ----- \n",
      " 0   level_0              int64 \n",
      " 1   index                int64 \n",
      " 2   date                 object\n",
      " 3   article_title        object\n",
      " 4   stock_symbol         object\n",
      " 5   article_title_clean  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 711.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d316eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/tqvd0k7104s47pv13lpjvlnh0000gn/T/ipykernel_10946/1123185437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"level_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_head' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"level_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "006e2645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [23:29<00:00, 44.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AAPL to fuzzy_keywords_90_labels_AAPL.csv\n",
      "\n",
      "ğŸ” Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [24:55<00:00, 46.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for MSFT to fuzzy_keywords_90_labels_MSFT.csv\n",
      "\n",
      "ğŸ” Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [18:09<00:00, 34.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for GOOGL to fuzzy_keywords_90_labels_GOOGL.csv\n",
      "\n",
      "ğŸ” Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [16:36<00:00, 31.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AMZN to fuzzy_keywords_90_labels_AMZN.csv\n",
      "\n",
      "ğŸ” Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [14:26<00:00, 27.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for NVDA to fuzzy_keywords_90_labels_NVDA.csv\n",
      "\n",
      "ğŸ” Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [13:01<00:00, 24.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for META to fuzzy_keywords_90_labels_META.csv\n",
      "\n",
      "ğŸ” Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [16:29<00:00, 30.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for TSLA to fuzzy_keywords_90_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 90\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\nğŸ” Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_90_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_90_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3078c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Harder-labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:35<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for AAPL to harder_label_AAPL.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:14<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for MSFT to harder_label_MSFT.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:20<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for GOOGL to harder_label_GOOGL.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:15<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for AMZN to harder_label_AMZN.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:04<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for NVDA to harder_label_NVDA.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:05<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for META to harder_label_META.csv\n",
      "\n",
      "ğŸ” Harder-labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:14<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for TSLA to harder_label_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hard match keywords (reuse fuzzy dict but with capitalization) ---\n",
    "hard_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"AAPL\", \"Apple\", \"Apple Inc\", \"Steve Jobs\", \"Tim Cook\", \"iPad\", \"iPhone\", \"Mac \", \"iOS \", \"Macintosh\",\n",
    "        \"AirPods\", \"Apple Watch\", \"Apple TV\", \"Apple Card\", \"Apple Pay\",\n",
    "        \"iCloud\", \"App Store\", \"Apple Music\", \"Wozniak\", \"Steve Wozniak\", \"Magnificent 7\",\n",
    "        \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"MSFT\", \"Microsoft\", \"Microsoft Office\", \"Windows\", \"Azure\", \"Xbox\", \"Bing\", \"LinkedIn\",\n",
    "        \"Visual Studio\", \"Microsoft Teams\", \"Microsoft 365\", \"Microsoft Dynamics\", \"Skype\",\n",
    "        \"OneDrive\", \"GitHub\", \"SharePoint\", \"Microsoft Viva\", \"Viva Engage\",\n",
    "        \"Satya Nadella\", \"Bill Gates\", \"Paul Allen\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"GOOGL\", \"GOOG\", \"Google\", \"Alphabet\", \"YouTube\", \"Gmail\", \"Android\", \"Chrome\", \"Google Maps\",\n",
    "        \"Google Cloud\", \"Google Drive\", \"abc.xyz\", \"Larry Page\", \"Sergey Brin\",\n",
    "        \"Sundar Pichai\", \"Ruth Porat\", \"Hennessy\", \"Ashkenazi\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"AMZN\", \"Amazon\", \"Amazon.com\", \" AWS \", \"Alexa\", \"Kindle\", \"Amazon Echo\", \"Amazon Prime\", \"EC2\",\n",
    "        \"Prime Video\", \"Twitch\", \"Audible\", \"Metro Goldwyn Mayer\", \"MGM Studios\", \"Fire Tablet\",\n",
    "        \"Jeff Bezos\", \"Bezos\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"NVDA\", \"NVIDIA\", \"Nvidia\", \"GeForce\", \"GeForce NOW\", \"CUDA\", \"NVIDIA RTX\", \"GTC\", \"Blackwell\",\n",
    "        \"NVIDIA DRIVE\", \"NVIDIA Jetson\", \"NVIDIA Isaac\", \"Tegra\", \"Quantum Computing\",\n",
    "        \"Jensen Huang\", \"Bill Dally\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"META\", \"Meta \", \"Meta Platforms\", \"Facebook\", \"Instagram\", \"WhatsApp\", \"Threads\",\n",
    "        \"Messenger\", \"Zuckerberg\", \"Mark Zuckerberg\", \"Meta Quest\", \"Metaverse\",\n",
    "        \"The Facebook Inc\", \"The Facebook\", \"facebook\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"TSLA\", \"Tesla\", \"Elon Musk\", \"Musk\", \"Model 3\", \"Model S\", \"Model X\", \"Cybertruck\",\n",
    "        \"Powerwall\", \"Megapack\", \"Solar City\", \"Tesla Semi\", \"Supercharger\",\n",
    "        \"Roadster\", \"SolarCity\", \"EV \", \"Electric Vehicle\", \"GigaFactory\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "CHUNKSIZE = 500_000\n",
    "TEXT_COLUMN = 'article_title'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index exists ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock and match keywords ---\n",
    "for stock, keywords in hard_keywords.items():\n",
    "    print(f\"\\nğŸ” Harder-labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"harder_label\": stock\n",
    "                })\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"harder_label_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved hard keyword labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43c3b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Harder-labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [05:01<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved hard keyword labels for NVDA to harder_fixed_label_NVDA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hard match keywords (reuse fuzzy dict but with capitalization) ---\n",
    "hard_keywords = {\n",
    "    \"NVDA\": [\n",
    "        \"NVDA\", \"NVIDIA\", \"Nvidia\", \"GeForce\", \"GeForce NOW\", \"CUDA\", \"NVIDIA RTX\", \"GTC\", \"Blackwell\",\n",
    "        \"NVIDIA DRIVE\", \"NVIDIA Jetson\", \"NVIDIA Isaac\", \"Tegra\", \"Quantum Computing\",\n",
    "        \"Jensen Huang\", \"Bill Dally\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Settings ---\n",
    "CHUNKSIZE = 500_000\n",
    "TEXT_COLUMN = 'article_title'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index exists ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock and match keywords ---\n",
    "for stock, keywords in hard_keywords.items():\n",
    "    print(f\"\\nğŸ” Harder-labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"harder_label\": stock\n",
    "                })\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"harder_fixed_label_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved hard keyword labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd22b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tesla\" in fuzzy_keywords[\"TSLA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e521fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [18:22<00:00, 34.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AAPL to fuzzy_keywords_80_labels_AAPL.csv\n",
      "\n",
      "ğŸ” Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [18:43<00:00, 35.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for MSFT to fuzzy_keywords_80_labels_MSFT.csv\n",
      "\n",
      "ğŸ” Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [20:45<00:00, 38.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for GOOGL to fuzzy_keywords_80_labels_GOOGL.csv\n",
      "\n",
      "ğŸ” Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [18:32<00:00, 34.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AMZN to fuzzy_keywords_80_labels_AMZN.csv\n",
      "\n",
      "ğŸ” Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [14:17<00:00, 26.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for NVDA to fuzzy_keywords_80_labels_NVDA.csv\n",
      "\n",
      "ğŸ” Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [12:44<00:00, 23.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for META to fuzzy_keywords_80_labels_META.csv\n",
      "\n",
      "ğŸ” Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [15:58<00:00, 29.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for TSLA to fuzzy_keywords_80_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 80\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\nğŸ” Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_80_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_80_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df876f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [22:20<00:00, 41.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AAPL to fuzzy_keywords_85_labels_AAPL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 85\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\nğŸ” Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_85_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_85_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45440aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [25:26<00:00, 47.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for MSFT to fuzzy_keywords_85_labels_MSFT.csv\n",
      "\n",
      "ğŸ” Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [19:38<00:00, 36.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for GOOGL to fuzzy_keywords_85_labels_GOOGL.csv\n",
      "\n",
      "ğŸ” Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [17:09<00:00, 32.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for AMZN to fuzzy_keywords_85_labels_AMZN.csv\n",
      "\n",
      "ğŸ” Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [14:45<00:00, 27.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for NVDA to fuzzy_keywords_85_labels_NVDA.csv\n",
      "\n",
      "ğŸ” Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [14:37<00:00, 27.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for META to fuzzy_keywords_85_labels_META.csv\n",
      "\n",
      "ğŸ” Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [17:12<00:00, 32.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy labels for TSLA to fuzzy_keywords_85_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 85\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\nğŸ” Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_85_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_85_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
