{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39135490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_large_csv_in_chunks(\n",
    "    file_path: str,\n",
    "    usecols: list[str] = None,\n",
    "    filter_col: str = None,\n",
    "    filter_values: list[str] = None,\n",
    "    chunksize: int = 100_000,\n",
    "    dropna_cols: list[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a large CSV file in chunks and returns a concatenated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    - usecols (list[str], optional): Columns to load.\n",
    "    - filter_col (str, optional): Column to apply filtering on.\n",
    "    - filter_values (list[str], optional): Values to keep in filter_col.\n",
    "    - chunksize (int): Number of rows per chunk.\n",
    "    - dropna_cols (list[str], optional): Drop rows with NaN in these columns.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered and loaded data in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = pd.read_csv(file_path, usecols=usecols, chunksize=chunksize)\n",
    "    chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(reader):\n",
    "        print(f\"🔄 Processing chunk {i + 1}\")\n",
    "        \n",
    "        if dropna_cols:\n",
    "            chunk = chunk.dropna(subset=dropna_cols)\n",
    "        \n",
    "        if filter_col and filter_values:\n",
    "            chunk = chunk[chunk[filter_col].isin(filter_values)]\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"✅ Loaded {len(df):,} rows into memory.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ce0f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 1\n",
      "🔄 Processing chunk 2\n",
      "🔄 Processing chunk 3\n",
      "🔄 Processing chunk 4\n",
      "🔄 Processing chunk 5\n",
      "🔄 Processing chunk 6\n",
      "🔄 Processing chunk 7\n",
      "🔄 Processing chunk 8\n",
      "🔄 Processing chunk 9\n",
      "🔄 Processing chunk 10\n",
      "🔄 Processing chunk 11\n",
      "🔄 Processing chunk 12\n",
      "🔄 Processing chunk 13\n",
      "🔄 Processing chunk 14\n",
      "🔄 Processing chunk 15\n",
      "🔄 Processing chunk 16\n",
      "🔄 Processing chunk 17\n",
      "🔄 Processing chunk 18\n",
      "🔄 Processing chunk 19\n",
      "🔄 Processing chunk 20\n",
      "🔄 Processing chunk 21\n",
      "🔄 Processing chunk 22\n",
      "🔄 Processing chunk 23\n",
      "🔄 Processing chunk 24\n",
      "🔄 Processing chunk 25\n",
      "🔄 Processing chunk 26\n",
      "🔄 Processing chunk 27\n",
      "🔄 Processing chunk 28\n",
      "🔄 Processing chunk 29\n",
      "🔄 Processing chunk 30\n",
      "🔄 Processing chunk 31\n",
      "🔄 Processing chunk 32\n",
      "🔄 Processing chunk 33\n",
      "🔄 Processing chunk 34\n",
      "🔄 Processing chunk 35\n",
      "🔄 Processing chunk 36\n",
      "🔄 Processing chunk 37\n",
      "🔄 Processing chunk 38\n",
      "🔄 Processing chunk 39\n",
      "🔄 Processing chunk 40\n",
      "🔄 Processing chunk 41\n",
      "🔄 Processing chunk 42\n",
      "🔄 Processing chunk 43\n",
      "🔄 Processing chunk 44\n",
      "🔄 Processing chunk 45\n",
      "🔄 Processing chunk 46\n",
      "🔄 Processing chunk 47\n",
      "🔄 Processing chunk 48\n",
      "🔄 Processing chunk 49\n",
      "🔄 Processing chunk 50\n",
      "🔄 Processing chunk 51\n",
      "🔄 Processing chunk 52\n",
      "🔄 Processing chunk 53\n",
      "🔄 Processing chunk 54\n",
      "🔄 Processing chunk 55\n",
      "🔄 Processing chunk 56\n",
      "🔄 Processing chunk 57\n",
      "🔄 Processing chunk 58\n",
      "🔄 Processing chunk 59\n",
      "🔄 Processing chunk 60\n",
      "🔄 Processing chunk 61\n",
      "🔄 Processing chunk 62\n",
      "🔄 Processing chunk 63\n",
      "🔄 Processing chunk 64\n",
      "🔄 Processing chunk 65\n",
      "🔄 Processing chunk 66\n",
      "🔄 Processing chunk 67\n",
      "🔄 Processing chunk 68\n",
      "🔄 Processing chunk 69\n",
      "🔄 Processing chunk 70\n",
      "🔄 Processing chunk 71\n",
      "🔄 Processing chunk 72\n",
      "🔄 Processing chunk 73\n",
      "🔄 Processing chunk 74\n",
      "🔄 Processing chunk 75\n",
      "🔄 Processing chunk 76\n",
      "🔄 Processing chunk 77\n",
      "🔄 Processing chunk 78\n",
      "🔄 Processing chunk 79\n",
      "🔄 Processing chunk 80\n",
      "🔄 Processing chunk 81\n",
      "🔄 Processing chunk 82\n",
      "🔄 Processing chunk 83\n",
      "🔄 Processing chunk 84\n",
      "🔄 Processing chunk 85\n",
      "🔄 Processing chunk 86\n",
      "🔄 Processing chunk 87\n",
      "🔄 Processing chunk 88\n",
      "🔄 Processing chunk 89\n",
      "🔄 Processing chunk 90\n",
      "🔄 Processing chunk 91\n",
      "🔄 Processing chunk 92\n",
      "🔄 Processing chunk 93\n",
      "🔄 Processing chunk 94\n",
      "🔄 Processing chunk 95\n",
      "🔄 Processing chunk 96\n",
      "🔄 Processing chunk 97\n",
      "🔄 Processing chunk 98\n",
      "🔄 Processing chunk 99\n",
      "🔄 Processing chunk 100\n",
      "🔄 Processing chunk 101\n",
      "🔄 Processing chunk 102\n",
      "🔄 Processing chunk 103\n",
      "🔄 Processing chunk 104\n",
      "🔄 Processing chunk 105\n",
      "🔄 Processing chunk 106\n",
      "🔄 Processing chunk 107\n",
      "🔄 Processing chunk 108\n",
      "🔄 Processing chunk 109\n",
      "🔄 Processing chunk 110\n",
      "🔄 Processing chunk 111\n",
      "🔄 Processing chunk 112\n",
      "🔄 Processing chunk 113\n",
      "🔄 Processing chunk 114\n",
      "🔄 Processing chunk 115\n",
      "🔄 Processing chunk 116\n",
      "🔄 Processing chunk 117\n",
      "🔄 Processing chunk 118\n",
      "🔄 Processing chunk 119\n",
      "🔄 Processing chunk 120\n",
      "🔄 Processing chunk 121\n",
      "🔄 Processing chunk 122\n",
      "🔄 Processing chunk 123\n",
      "🔄 Processing chunk 124\n",
      "🔄 Processing chunk 125\n",
      "🔄 Processing chunk 126\n",
      "🔄 Processing chunk 127\n",
      "🔄 Processing chunk 128\n",
      "🔄 Processing chunk 129\n",
      "🔄 Processing chunk 130\n",
      "🔄 Processing chunk 131\n",
      "🔄 Processing chunk 132\n",
      "🔄 Processing chunk 133\n",
      "🔄 Processing chunk 134\n",
      "🔄 Processing chunk 135\n",
      "🔄 Processing chunk 136\n",
      "🔄 Processing chunk 137\n",
      "🔄 Processing chunk 138\n",
      "🔄 Processing chunk 139\n",
      "🔄 Processing chunk 140\n",
      "🔄 Processing chunk 141\n",
      "🔄 Processing chunk 142\n",
      "🔄 Processing chunk 143\n",
      "🔄 Processing chunk 144\n",
      "🔄 Processing chunk 145\n",
      "🔄 Processing chunk 146\n",
      "🔄 Processing chunk 147\n",
      "🔄 Processing chunk 148\n",
      "🔄 Processing chunk 149\n",
      "🔄 Processing chunk 150\n",
      "🔄 Processing chunk 151\n",
      "🔄 Processing chunk 152\n",
      "🔄 Processing chunk 153\n",
      "🔄 Processing chunk 154\n",
      "🔄 Processing chunk 155\n",
      "🔄 Processing chunk 156\n",
      "✅ Loaded 15,549,298 rows into memory.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Volumes/T7/External Downloads/nasdaq_titles_fuzzy_rdy.csv\"\n",
    "\n",
    "df = load_large_csv_in_chunks(\n",
    "    usecols= [\"date\", \"article_title\", \"article_title_clean\", \"stock_symbol\"],\n",
    "    file_path=file_path,\n",
    "    chunksize = 100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25affe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15549298 entries, 0 to 15549297\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Dtype \n",
      "---  ------               ----- \n",
      " 0   level_0              int64 \n",
      " 1   index                int64 \n",
      " 2   date                 object\n",
      " 3   article_title        object\n",
      " 4   stock_symbol         object\n",
      " 5   article_title_clean  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 711.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d316eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j5/tqvd0k7104s47pv13lpjvlnh0000gn/T/ipykernel_10946/1123185437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"level_0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_head' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"level_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "006e2645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|█████████████████████████████████████| 32/32 [23:29<00:00, 44.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AAPL to fuzzy_keywords_90_labels_AAPL.csv\n",
      "\n",
      "🔍 Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|█████████████████████████████████████| 32/32 [24:55<00:00, 46.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for MSFT to fuzzy_keywords_90_labels_MSFT.csv\n",
      "\n",
      "🔍 Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|████████████████████████████████████| 32/32 [18:09<00:00, 34.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for GOOGL to fuzzy_keywords_90_labels_GOOGL.csv\n",
      "\n",
      "🔍 Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|█████████████████████████████████████| 32/32 [16:36<00:00, 31.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AMZN to fuzzy_keywords_90_labels_AMZN.csv\n",
      "\n",
      "🔍 Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|█████████████████████████████████████| 32/32 [14:26<00:00, 27.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for NVDA to fuzzy_keywords_90_labels_NVDA.csv\n",
      "\n",
      "🔍 Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|█████████████████████████████████████| 32/32 [13:01<00:00, 24.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for META to fuzzy_keywords_90_labels_META.csv\n",
      "\n",
      "🔍 Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|█████████████████████████████████████| 32/32 [16:29<00:00, 30.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for TSLA to fuzzy_keywords_90_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 90\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\n🔍 Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_90_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_90_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3078c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Harder-labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|█████████████████████████████████████| 32/32 [01:35<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for AAPL to harder_label_AAPL.csv\n",
      "\n",
      "🔍 Harder-labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|█████████████████████████████████████| 32/32 [01:14<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for MSFT to harder_label_MSFT.csv\n",
      "\n",
      "🔍 Harder-labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|████████████████████████████████████| 32/32 [01:20<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for GOOGL to harder_label_GOOGL.csv\n",
      "\n",
      "🔍 Harder-labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|█████████████████████████████████████| 32/32 [01:15<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for AMZN to harder_label_AMZN.csv\n",
      "\n",
      "🔍 Harder-labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|█████████████████████████████████████| 32/32 [01:04<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for NVDA to harder_label_NVDA.csv\n",
      "\n",
      "🔍 Harder-labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|█████████████████████████████████████| 32/32 [01:05<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for META to harder_label_META.csv\n",
      "\n",
      "🔍 Harder-labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|█████████████████████████████████████| 32/32 [01:14<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for TSLA to harder_label_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hard match keywords (reuse fuzzy dict but with capitalization) ---\n",
    "hard_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"AAPL\", \"Apple\", \"Apple Inc\", \"Steve Jobs\", \"Tim Cook\", \"iPad\", \"iPhone\", \"Mac \", \"iOS \", \"Macintosh\",\n",
    "        \"AirPods\", \"Apple Watch\", \"Apple TV\", \"Apple Card\", \"Apple Pay\",\n",
    "        \"iCloud\", \"App Store\", \"Apple Music\", \"Wozniak\", \"Steve Wozniak\", \"Magnificent 7\",\n",
    "        \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"MSFT\", \"Microsoft\", \"Microsoft Office\", \"Windows\", \"Azure\", \"Xbox\", \"Bing\", \"LinkedIn\",\n",
    "        \"Visual Studio\", \"Microsoft Teams\", \"Microsoft 365\", \"Microsoft Dynamics\", \"Skype\",\n",
    "        \"OneDrive\", \"GitHub\", \"SharePoint\", \"Microsoft Viva\", \"Viva Engage\",\n",
    "        \"Satya Nadella\", \"Bill Gates\", \"Paul Allen\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"GOOGL\", \"GOOG\", \"Google\", \"Alphabet\", \"YouTube\", \"Gmail\", \"Android\", \"Chrome\", \"Google Maps\",\n",
    "        \"Google Cloud\", \"Google Drive\", \"abc.xyz\", \"Larry Page\", \"Sergey Brin\",\n",
    "        \"Sundar Pichai\", \"Ruth Porat\", \"Hennessy\", \"Ashkenazi\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"AMZN\", \"Amazon\", \"Amazon.com\", \" AWS \", \"Alexa\", \"Kindle\", \"Amazon Echo\", \"Amazon Prime\", \"EC2\",\n",
    "        \"Prime Video\", \"Twitch\", \"Audible\", \"Metro Goldwyn Mayer\", \"MGM Studios\", \"Fire Tablet\",\n",
    "        \"Jeff Bezos\", \"Bezos\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"NVDA\", \"NVIDIA\", \"Nvidia\", \"GeForce\", \"GeForce NOW\", \"CUDA\", \"NVIDIA RTX\", \"GTC\", \"Blackwell\",\n",
    "        \"NVIDIA DRIVE\", \"NVIDIA Jetson\", \"NVIDIA Isaac\", \"Tegra\", \"Quantum Computing\",\n",
    "        \"Jensen Huang\", \"Bill Dally\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"META\", \"Meta \", \"Meta Platforms\", \"Facebook\", \"Instagram\", \"WhatsApp\", \"Threads\",\n",
    "        \"Messenger\", \"Zuckerberg\", \"Mark Zuckerberg\", \"Meta Quest\", \"Metaverse\",\n",
    "        \"The Facebook Inc\", \"The Facebook\", \"facebook\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\", \"FAANG\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"TSLA\", \"Tesla\", \"Elon Musk\", \"Musk\", \"Model 3\", \"Model S\", \"Model X\", \"Cybertruck\",\n",
    "        \"Powerwall\", \"Megapack\", \"Solar City\", \"Tesla Semi\", \"Supercharger\",\n",
    "        \"Roadster\", \"SolarCity\", \"EV \", \"Electric Vehicle\", \"GigaFactory\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "CHUNKSIZE = 500_000\n",
    "TEXT_COLUMN = 'article_title'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index exists ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock and match keywords ---\n",
    "for stock, keywords in hard_keywords.items():\n",
    "    print(f\"\\n🔍 Harder-labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"harder_label\": stock\n",
    "                })\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"harder_label_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved hard keyword labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43c3b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Harder-labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|█████████████████████████████████████| 32/32 [05:01<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved hard keyword labels for NVDA to harder_fixed_label_NVDA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Hard match keywords (reuse fuzzy dict but with capitalization) ---\n",
    "hard_keywords = {\n",
    "    \"NVDA\": [\n",
    "        \"NVDA\", \"NVIDIA\", \"Nvidia\", \"GeForce\", \"GeForce NOW\", \"CUDA\", \"NVIDIA RTX\", \"GTC\", \"Blackwell\",\n",
    "        \"NVIDIA DRIVE\", \"NVIDIA Jetson\", \"NVIDIA Isaac\", \"Tegra\", \"Quantum Computing\",\n",
    "        \"Jensen Huang\", \"Bill Dally\", \"Magnificent 7\", \"Magnificent Seven\", \"MAG7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Settings ---\n",
    "CHUNKSIZE = 500_000\n",
    "TEXT_COLUMN = 'article_title'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index exists ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock and match keywords ---\n",
    "for stock, keywords in hard_keywords.items():\n",
    "    print(f\"\\n🔍 Harder-labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                matches.append({\n",
    "                    \"index\": idx,\n",
    "                    \"harder_label\": stock\n",
    "                })\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"harder_fixed_label_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved hard keyword labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd22b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tesla\" in fuzzy_keywords[\"TSLA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e521fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|█████████████████████████████████████| 32/32 [18:22<00:00, 34.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AAPL to fuzzy_keywords_80_labels_AAPL.csv\n",
      "\n",
      "🔍 Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|█████████████████████████████████████| 32/32 [18:43<00:00, 35.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for MSFT to fuzzy_keywords_80_labels_MSFT.csv\n",
      "\n",
      "🔍 Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|████████████████████████████████████| 32/32 [20:45<00:00, 38.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for GOOGL to fuzzy_keywords_80_labels_GOOGL.csv\n",
      "\n",
      "🔍 Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|█████████████████████████████████████| 32/32 [18:32<00:00, 34.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AMZN to fuzzy_keywords_80_labels_AMZN.csv\n",
      "\n",
      "🔍 Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|█████████████████████████████████████| 32/32 [14:17<00:00, 26.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for NVDA to fuzzy_keywords_80_labels_NVDA.csv\n",
      "\n",
      "🔍 Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|█████████████████████████████████████| 32/32 [12:44<00:00, 23.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for META to fuzzy_keywords_80_labels_META.csv\n",
      "\n",
      "🔍 Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|█████████████████████████████████████| 32/32 [15:58<00:00, 29.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for TSLA to fuzzy_keywords_80_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 80\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\n🔍 Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_80_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_80_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df876f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Labeling for: AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AAPL: 100%|█████████████████████████████████████| 32/32 [22:20<00:00, 41.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AAPL to fuzzy_keywords_85_labels_AAPL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"AAPL\": [\n",
    "        \"aapl\", \"apple\", \"apple inc\", \"steve jobs\", \"tim cook\", \"ipad\", \"iphone\", \"mac \", \"ios \", \"macintosh\",\n",
    "        \"airpods\", \"apple watch\", \"apple tv\", \"apple card\", \"apple pay\",\n",
    "        \"icloud\", \"app store\", \"apple music\", \"wozniak\", \"steve wozniak\", \"magnificent 7\",\n",
    "        \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 85\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\n🔍 Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_85_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_85_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45440aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Labeling for: MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSFT: 100%|█████████████████████████████████████| 32/32 [25:26<00:00, 47.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for MSFT to fuzzy_keywords_85_labels_MSFT.csv\n",
      "\n",
      "🔍 Labeling for: GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GOOGL: 100%|████████████████████████████████████| 32/32 [19:38<00:00, 36.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for GOOGL to fuzzy_keywords_85_labels_GOOGL.csv\n",
      "\n",
      "🔍 Labeling for: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMZN: 100%|█████████████████████████████████████| 32/32 [17:09<00:00, 32.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for AMZN to fuzzy_keywords_85_labels_AMZN.csv\n",
      "\n",
      "🔍 Labeling for: NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NVDA: 100%|█████████████████████████████████████| 32/32 [14:45<00:00, 27.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for NVDA to fuzzy_keywords_85_labels_NVDA.csv\n",
      "\n",
      "🔍 Labeling for: META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "META: 100%|█████████████████████████████████████| 32/32 [14:37<00:00, 27.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for META to fuzzy_keywords_85_labels_META.csv\n",
      "\n",
      "🔍 Labeling for: TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSLA: 100%|█████████████████████████████████████| 32/32 [17:12<00:00, 32.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fuzzy labels for TSLA to fuzzy_keywords_85_labels_TSLA.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Define your fuzzy keyword dictionary ---\n",
    "fuzzy_keywords = {\n",
    "    \"MSFT\": [\n",
    "        \"msft\", \"microsoft\", \"microsoft office\", \"windows\", \"azure\", \"xbox\", \"bing\", \"linkedin\",\n",
    "        \"visual studio\", \"microsoft teams\", \"microsoft 365\", \"microsoft dynamics\", \"skype\",\n",
    "        \"onedrive\", \"github\", \"sharepoint\", \"microsoft viva\", \"viva engage\",\n",
    "        \"satya nadella\", \"bill gates\", \"paul allen\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"GOOGL\": [\n",
    "        \"googl\", \"goog\", \"google\", \"alphabet\", \"youtube\", \"gmail\", \"android\", \"chrome\", \"google maps\",\n",
    "        \"google cloud\", \"google drive\", \"abc.xyz\", \"larry page\", \"sergey brin\",\n",
    "        \"sundar pichai\", \"ruth porat\", \"hennessy\", \"ashkenazi\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\" \n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        \"amzn\", \"amazon\", \"amazon.com\", \" aws \", \"alexa\", \"kindle\", \"amazon echo\", \"amazon prime\", \"ec2\",\n",
    "        \"prime video\", \"twitch\", \"audible\", \"metro goldwyn mayer\", \"mgm studios\", \"fire tablet\",\n",
    "        \"jeff bezos\", \"bezos\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        \"nvda\", \"nvidia\", \"geforce\", \"geforce now\", \"cuda\", \"nvidia rtx\", \"gtc\", \"blackwell\",\n",
    "        \"nvidia drive\", \"nvidia jetson\", \"nvidia isaac\", \"tegra\", \"quantum computing\",\n",
    "        \"jensen huang\", \"bill dally\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ],\n",
    "    \"META\": [\n",
    "        \"meta \", \"meta platforms\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"messenger\", \"zuckerberg\", \"mark zuckerberg\", \"meta quest\", \"metaverse\",\n",
    "        \"the facebook inc\", \"magnificent 7\", \"magnificent seven\", \"mag7\", \"faang\"\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        \"tsla\", \"tesla\", \"elon musk\", \"musk\", \"model 3\", \"model s \", \"model x \", \"cybertruck\",\n",
    "        \"powerwall\", \"megapack\", \"solar city\", \"tesla semi\", \"supercharger\",\n",
    "        \"roadster\", \"solarcity\", \"electric vehicle\", \"gigafactory\", \"magnificent 7\", \"magnificent seven\", \"mag7\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "FUZZY_THRESHOLD = 85\n",
    "CHUNKSIZE = 500_000  # adapt based on available RAM\n",
    "TEXT_COLUMN = 'article_title_clean'\n",
    "INDEX_COLUMN = 'index'\n",
    "\n",
    "# --- Make sure index is set ---\n",
    "#df = df.reset_index(drop=False)\n",
    "\n",
    "# --- Loop through each stock individually ---\n",
    "for stock, keywords in fuzzy_keywords.items():\n",
    "    print(f\"\\n🔍 Labeling for: {stock}\")\n",
    "    matches = []\n",
    "\n",
    "    # Process in chunks\n",
    "    for start in tqdm(range(0, len(df), CHUNKSIZE), desc=f\"{stock}\"):\n",
    "        end = min(start + CHUNKSIZE, len(df))\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        for idx, text in zip(chunk[INDEX_COLUMN], chunk[TEXT_COLUMN]):\n",
    "            if pd.isna(text): continue\n",
    "\n",
    "            for keyword in keywords:\n",
    "                score = fuzz.partial_ratio(keyword, text)\n",
    "                if score >= FUZZY_THRESHOLD:\n",
    "                    matches.append({\n",
    "                        \"index\": idx,\n",
    "                        \"fuzzy_85_label\": stock\n",
    "                    })\n",
    "                    break  # avoid double-labeling the same article for this stock\n",
    "\n",
    "    # Save to CSV\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "    output_path = f\"fuzzy_keywords_85_labels_{stock}.csv\"\n",
    "    matches_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved fuzzy labels for {stock} to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
